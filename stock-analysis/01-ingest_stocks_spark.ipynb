{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c50cc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-06-21 09:14:23,713 [info] loaded project stocks from MLRun DB\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "project = mlrun.get_or_create_project(name='stocks',user_project=True, context=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634eacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlrun.runtimes import Spark3Runtime\n",
    "# Spark3Runtime.deploy_default_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee224f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun: start-code\n",
    "from mlrun.feature_store.api import ingest\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def ingest_handler(context):\n",
    "    ingest(mlrun_context=context) # The handler function must call ingest with the mlrun_context\n",
    "\n",
    "# You can add your own PySpark code as a graph step:\n",
    "def filter_func(df, context=None):\n",
    "    return df.filter(\"bid>55\") # PySpark code\n",
    "\n",
    "\n",
    "def add_column(df, context=None):\n",
    "    return_df = df.withColumn(\"bid_ask_diff\", df.bid-df.ask)\n",
    "    return return_df\n",
    "\n",
    "def print_dataframe(df, contest=None):\n",
    "    print(\"type of data frame {}\".format(type(df)))        \n",
    "    print(df.show())\n",
    "    return df\n",
    "\n",
    "# mlrun: end-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38eb91c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/User/aviaIguazio/demos/stock-analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "def get_stocks_file(directory):        \n",
    "    # iterate over files in\n",
    "    # that directory\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            if f.endswith('StocksGenerator.csv'):                                                \n",
    "                path = f.replace(\"/v3io\",\"v3io://\")\n",
    "                print(path)\n",
    "                return path            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b06de174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v3io:///projects/data/20220621-092056-StocksGenerator.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.serving.states.TaskStep at 0x7f0ec061c3d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun.datastore.sources import CSVSource\n",
    "from mlrun import code_to_function\n",
    "import mlrun.feature_store as fstore\n",
    "\n",
    "feature_set = fstore.FeatureSet(\"stocks\", entities=[fstore.Entity(\"ticker\")],timestamp_key='Datetime', engine=\"spark\")\n",
    "\n",
    "#source = CSVSource(\"mycsv\", path=\"v3io:///users/aviaIguazio/demos/stock-analysis/data/20220621-084843-StocksGenerator.csv\")\n",
    "source_path = get_stocks_file('/v3io/projects/data')            \n",
    "source = CSVSource(\"mycsv\", path=source_path)\n",
    "\n",
    "feature_set.graph\\\n",
    "    .to(name=\"print_dataframe\", handler=\"print_dataframe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fc45f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"396pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 396.48 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-94 392.4837,-94 392.4837,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"38.5476,-27.0493 40.698,-27.1479 42.8263,-27.2953 44.9236,-27.4913 46.9815,-27.7353 48.9917,-28.0266 50.9463,-28.3645 52.8377,-28.7479 54.6587,-29.1759 56.4025,-29.6472 58.0628,-30.1606 59.634,-30.7147 61.1107,-31.308 62.4882,-31.9388 63.7625,-32.6054 64.9302,-33.3059 65.9882,-34.0385 66.9343,-34.8012 67.7669,-35.5918 68.4849,-36.4082 69.0878,-37.2481 69.5758,-38.1093 69.9496,-38.9894 70.2102,-39.886 70.3595,-40.7965 70.3997,-41.7186 70.3334,-42.6497 70.1636,-43.5873 69.8937,-44.5287 69.5276,-45.4713 69.0691,-46.4127 68.5225,-47.3503 67.8923,-48.2814 67.1831,-49.2035 66.3996,-50.114 65.5464,-51.0106 64.6285,-51.8907 63.6504,-52.7519 62.617,-53.5918 61.5329,-54.4082 60.4024,-55.1988 59.2299,-55.9615 58.0197,-56.6941 56.7755,-57.3946 55.5012,-58.0612 54.2002,-58.692 52.8757,-59.2853 51.5309,-59.8394 50.1684,-60.3528 48.7908,-60.8241 47.4003,-61.2521 45.9989,-61.6355 44.5886,-61.9734 43.1708,-62.2647 41.7472,-62.5087 40.3189,-62.7047 38.8872,-62.8521 37.4531,-62.9507 36.0175,-63 34.5815,-63 33.146,-62.9507 31.7119,-62.8521 30.2801,-62.7047 28.8519,-62.5087 27.4282,-62.2647 26.0105,-61.9734 24.6001,-61.6355 23.1988,-61.2521 21.8083,-60.8241 20.4306,-60.3528 19.0681,-59.8394 17.7233,-59.2853 16.3989,-58.692 15.0979,-58.0612 13.8236,-57.3946 12.5794,-56.6941 11.3691,-55.9615 10.1967,-55.1988 9.0662,-54.4082 7.982,-53.5918 6.9486,-52.7519 5.9706,-51.8907 5.0526,-51.0106 4.1995,-50.114 3.4159,-49.2035 2.7067,-48.2814 2.0765,-47.3503 1.53,-46.4127 1.0715,-45.4713 .7053,-44.5287 .4355,-43.5873 .2657,-42.6497 .1993,-41.7186 .2395,-40.7965 .3888,-39.886 .6495,-38.9894 1.0232,-38.1093 1.5112,-37.2481 2.1141,-36.4082 2.8321,-35.5918 3.6647,-34.8012 4.6109,-34.0385 5.6689,-33.3059 6.8365,-32.6054 8.1108,-31.9388 9.4884,-31.308 10.9651,-30.7147 12.5362,-30.1606 14.1966,-29.6472 15.9404,-29.1759 17.7614,-28.7479 19.6528,-28.3645 21.6074,-28.0266 23.6176,-27.7353 25.6755,-27.4913 27.7728,-27.2953 29.901,-27.1479 32.0515,-27.0493 34.2154,-27 36.3837,-27 38.5476,-27.0493\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.2995\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">start</text>\n",
       "</g>\n",
       "<!-- print_dataframe -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>print_dataframe</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"193.0414\" cy=\"-45\" rx=\"86.3847\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.0414\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">print_dataframe</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;print_dataframe -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;print_dataframe</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M69.9166,-45C77.9754,-45 86.9455,-45 96.268,-45\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.4956,-48.5001 106.4956,-45 96.4955,-41.5001 96.4956,-48.5001\"/>\n",
       "</g>\n",
       "<!-- parquet -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>parquet</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M388.4837,-86.7273C388.4837,-88.5331 372.1238,-90 351.9837,-90 331.8436,-90 315.4837,-88.5331 315.4837,-86.7273 315.4837,-86.7273 315.4837,-57.2727 315.4837,-57.2727 315.4837,-55.4669 331.8436,-54 351.9837,-54 372.1238,-54 388.4837,-55.4669 388.4837,-57.2727 388.4837,-57.2727 388.4837,-86.7273 388.4837,-86.7273\"/>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M388.4837,-86.7273C388.4837,-84.9214 372.1238,-83.4545 351.9837,-83.4545 331.8436,-83.4545 315.4837,-84.9214 315.4837,-86.7273\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.9837\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">parquet</text>\n",
       "</g>\n",
       "<!-- print_dataframe&#45;&gt;parquet -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>print_dataframe&#45;&gt;parquet</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M260.4472,-56.4504C275.5618,-59.018 291.2725,-61.6868 305.3234,-64.0737\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"304.7841,-67.5321 315.229,-65.7564 305.9564,-60.631 304.7841,-67.5321\"/>\n",
       "</g>\n",
       "<!-- nosql -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>nosql</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M379.4837,-32.7273C379.4837,-34.5331 367.1578,-36 351.9837,-36 336.8097,-36 324.4837,-34.5331 324.4837,-32.7273 324.4837,-32.7273 324.4837,-3.2727 324.4837,-3.2727 324.4837,-1.4669 336.8097,0 351.9837,0 367.1578,0 379.4837,-1.4669 379.4837,-3.2727 379.4837,-3.2727 379.4837,-32.7273 379.4837,-32.7273\"/>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M379.4837,-32.7273C379.4837,-30.9214 367.1578,-29.4545 351.9837,-29.4545 336.8097,-29.4545 324.4837,-30.9214 324.4837,-32.7273\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.9837\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nosql</text>\n",
       "</g>\n",
       "<!-- print_dataframe&#45;&gt;nosql -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>print_dataframe&#45;&gt;nosql</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M260.4472,-33.5496C278.9092,-30.4134 298.2605,-27.1261 314.39,-24.3862\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"315.2098,-27.7971 324.4824,-22.6717 314.0374,-20.896 315.2098,-27.7971\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f0ec05b66d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "# Setting default targets (nosql & parquet)\n",
    "feature_set.set_targets(with_defaults=True) \n",
    "feature_set.plot(rankdir=\"LR\", with_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb2862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-06-21 09:24:33,196 [info] starting run stocks-ingest uid=09c3192402494efdbaeac9af8707ff18 DB=http://mlrun-api:8080\n",
      "++ id -u\n",
      "+ myuid=1000\n",
      "++ id -g\n",
      "+ mygid=1000\n",
      "+ set +e\n",
      "++ getent passwd 1000\n",
      "+ uidentry=iguazio:x:1000:1000::/igz:/bin/bash\n",
      "+ set -e\n",
      "+ '[' -z iguazio:x:1000:1000::/igz:/bin/bash ']'\n",
      "+ SPARK_CLASSPATH=':/spark/jars/*'\n",
      "+ env\n",
      "+ grep SPARK_JAVA_OPT_\n",
      "+ sort -t_ -k4 -n\n",
      "+ sed 's/[^=]*=\\(.*\\)/\\1/g'\n",
      "+ readarray -t SPARK_EXECUTOR_JAVA_OPTS\n",
      "+ '[' -n '' ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -n /hadoop ']'\n",
      "+ '[' -z '' ']'\n",
      "++ /hadoop/bin/hadoop classpath\n",
      "+ export 'SPARK_DIST_CLASSPATH=/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/lib/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.2.0.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar'\n",
      "+ SPARK_DIST_CLASSPATH='/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/lib/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.2.0.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/opt/spark/conf:/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ case \"$1\" in\n",
      "+ shift 1\n",
      "+ CMD=(\"$SPARK_HOME/bin/spark-submit\" --conf \"spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS\" --deploy-mode client \"$@\")\n",
      "+ exec /usr/bin/tini -s -- /spark/bin/spark-submit --conf spark.driver.bindAddress=10.200.156.168 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner local:///etc/config/mlrun/spark-function-code.py\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2022-06-21 09:24:41,787 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "> 2022-06-21 09:24:48,890 [info] starting ingestion task to store://feature-sets/stocks-avia/stocks:latest.\n",
      "2022-06-21 09:24:49,479 INFO spark.SparkContext: Running Spark version 3.1.2\n",
      "2022-06-21 09:24:49,532 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-06-21 09:24:49,533 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2022-06-21 09:24:49,533 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-06-21 09:24:49,533 INFO spark.SparkContext: Submitted application: stocks-ingest-09c3192402494efdbaeac9af8707ff18\n",
      "2022-06-21 09:24:49,574 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2022-06-21 09:24:49,592 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2022-06-21 09:24:49,594 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2022-06-21 09:24:49,683 INFO spark.SecurityManager: Changing view acls to: iguazio\n",
      "2022-06-21 09:24:49,683 INFO spark.SecurityManager: Changing modify acls to: iguazio\n",
      "2022-06-21 09:24:49,683 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2022-06-21 09:24:49,683 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2022-06-21 09:24:49,683 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(iguazio); groups with view permissions: Set(); users  with modify permissions: Set(iguazio); groups with modify permissions: Set()\n",
      "2022-06-21 09:24:49,957 INFO util.Utils: Successfully started service 'sparkDriver' on port 7078.\n",
      "2022-06-21 09:24:49,982 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2022-06-21 09:24:50,014 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2022-06-21 09:24:50,037 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2022-06-21 09:24:50,038 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2022-06-21 09:24:50,041 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2022-06-21 09:24:50,054 INFO storage.DiskBlockManager: Created local directory at /var/data/spark-2306d23c-2e03-47d6-b219-22f98f3bab7c/blockmgr-271f718e-c119-4cf7-9a26-5d244581325e\n",
      "2022-06-21 09:24:50,078 INFO memory.MemoryStore: MemoryStore started with capacity 413.9 MiB\n",
      "2022-06-21 09:24:50,093 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2022-06-21 09:24:50,191 INFO util.log: Logging initialized @10420ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2022-06-21 09:24:50,273 INFO server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-LTS\n",
      "2022-06-21 09:24:50,292 INFO server.Server: Started @10522ms\n",
      "2022-06-21 09:24:50,328 INFO server.AbstractConnector: Started ServerConnector@67c0e878{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2022-06-21 09:24:50,328 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2022-06-21 09:24:50,352 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@76e2406c{/jobs,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,355 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@117df82b{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,355 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e263ce4{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,356 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a182169{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,357 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2445f1d8{/stages,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,358 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3d382ee3{/stages/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,358 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@dced72c{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,360 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e46808e{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,360 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3361437c{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,361 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c823497{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,362 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a8989b9{/storage,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,362 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@158bd443{/storage/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,363 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32d24931{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,364 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7271f807{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,364 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60ba64f0{/environment,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,365 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c918d7b{/environment/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,365 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46477a66{/executors,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,366 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58daf549{/executors/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,367 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18095ef8{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,368 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7db75a5e{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,376 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e79f9b0{/static,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,377 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50b62f3d{/,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,378 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d88aac0{/api,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,379 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@aee9531{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,379 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@585cc41d{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:50,381 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:4040\n",
      "2022-06-21 09:24:50,402 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-hcfs_2.12.jar at file:/spark/v3io-libs/v3io-hcfs_2.12.jar with timestamp 1655803489470\n",
      "2022-06-21 09:24:50,402 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-streaming_2.12.jar at file:/spark/v3io-libs/v3io-spark3-streaming_2.12.jar with timestamp 1655803489470\n",
      "2022-06-21 09:24:50,402 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar at file:/spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar with timestamp 1655803489470\n",
      "2022-06-21 09:24:50,402 INFO spark.SparkContext: Added JAR local:///igz/java/libs/scala-library-2.12.14.jar at file:/igz/java/libs/scala-library-2.12.14.jar with timestamp 1655803489470\n",
      "2022-06-21 09:24:50,402 INFO spark.SparkContext: Added JAR local:///spark/jars/jmx_prometheus_javaagent-0.16.1.jar at file:/spark/jars/jmx_prometheus_javaagent-0.16.1.jar with timestamp 1655803489470\n",
      "2022-06-21 09:24:50,403 WARN spark.SparkContext: File with 'local' scheme local:///igz/java/libs/v3io-pyspark.zip is not supported to add to file server, since it is already available on every node.\n",
      "2022-06-21 09:24:50,511 INFO k8s.SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n",
      "2022-06-21 09:24:51,629 INFO k8s.ExecutorPodsAllocator: Going to request 2 executors from Kubernetes for ResourceProfile Id: 0, target: 2 running: 0.\n",
      "2022-06-21 09:24:51,702 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-06-21 09:24:51,755 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.\n",
      "2022-06-21 09:24:51,755 INFO netty.NettyBlockTransferService: Server created on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079\n",
      "2022-06-21 09:24:51,756 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2022-06-21 09:24:51,765 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-06-21 09:24:51,769 INFO storage.BlockManagerMasterEndpoint: Registering block manager stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 with 413.9 MiB RAM, BlockManagerId(driver, stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-06-21 09:24:51,771 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-06-21 09:24:51,773 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-06-21 09:24:51,839 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-06-21 09:24:51,849 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b8d9a1a{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:58,016 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.156.170:60200) with ID 2,  ResourceProfileId 0\n",
      "2022-06-21 09:24:58,019 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.156.169:48188) with ID 1,  ResourceProfileId 0\n",
      "2022-06-21 09:24:58,051 INFO k8s.KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2022-06-21 09:24:58,090 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.156.169:45245 with 413.9 MiB RAM, BlockManagerId(1, 10.200.156.169, 45245, None)\n",
      "2022-06-21 09:24:58,091 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.156.170:35961 with 413.9 MiB RAM, BlockManagerId(2, 10.200.156.170, 35961, None)\n",
      "2022-06-21 09:24:58,361 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/spark/spark-warehouse').\n",
      "2022-06-21 09:24:58,362 INFO internal.SharedState: Warehouse path is 'file:/spark/spark-warehouse'.\n",
      "2022-06-21 09:24:58,378 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28fdcd48{/SQL,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:58,379 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2042cfa2{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:58,379 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6edda90e{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:58,380 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68d57edb{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:58,397 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40860e59{/static/sql,null,AVAILABLE,@Spark}\n",
      "2022-06-21 09:24:59,934 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "2022-06-21 09:25:00,754 INFO datasources.InMemoryFileIndex: It took 217 ms to list leaf files for 1 paths.\n",
      "2022-06-21 09:25:00,926 INFO datasources.InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
      "2022-06-21 09:25:03,165 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-21 09:25:03,168 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "2022-06-21 09:25:03,171 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2022-06-21 09:25:03,796 INFO codegen.CodeGenerator: Code generated in 267.954302 ms\n",
      "2022-06-21 09:25:03,860 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 250.3 KiB, free 413.7 MiB)\n",
      "2022-06-21 09:25:03,934 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 413.6 MiB)\n",
      "2022-06-21 09:25:03,937 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:03,939 INFO spark.SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0\n",
      "2022-06-21 09:25:03,949 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-21 09:25:04,082 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "2022-06-21 09:25:04,098 INFO scheduler.DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-06-21 09:25:04,098 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "2022-06-21 09:25:04,098 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-06-21 09:25:04,099 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-06-21 09:25:04,104 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-06-21 09:25:04,176 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 413.6 MiB)\n",
      "2022-06-21 09:25:04,179 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 413.6 MiB)\n",
      "2022-06-21 09:25:04,180 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 (size: 5.4 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:04,180 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-21 09:25:04,194 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-21 09:25:04,195 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2022-06-21 09:25:04,254 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.200.156.169, executor 1, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()\n",
      "2022-06-21 09:25:04,506 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.200.156.169:45245 (size: 5.4 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:05,360 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.156.169:45245 (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:07,160 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2916 ms on 10.200.156.169 (executor 1) (1/1)\n",
      "2022-06-21 09:25:07,162 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2022-06-21 09:25:07,170 INFO scheduler.DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 3.041 s\n",
      "2022-06-21 09:25:07,174 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-21 09:25:07,174 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "2022-06-21 09:25:07,176 INFO scheduler.DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 3.093376 s\n",
      "2022-06-21 09:25:07,200 INFO codegen.CodeGenerator: Code generated in 11.258084 ms\n",
      "2022-06-21 09:25:07,255 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-21 09:25:07,255 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-21 09:25:07,255 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2022-06-21 09:25:07,263 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 250.3 KiB, free 413.4 MiB)\n",
      "2022-06-21 09:25:07,298 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 413.3 MiB)\n",
      "2022-06-21 09:25:07,327 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:07,328 INFO spark.SparkContext: Created broadcast 2 from load at NativeMethodAccessorImpl.java:0\n",
      "2022-06-21 09:25:07,329 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-21 09:25:07,331 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 in memory (size: 5.4 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:07,340 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.200.156.169:45245 in memory (size: 5.4 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:07,392 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "2022-06-21 09:25:07,393 INFO scheduler.DAGScheduler: Got job 1 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-06-21 09:25:07,393 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (load at NativeMethodAccessorImpl.java:0)\n",
      "2022-06-21 09:25:07,393 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-06-21 09:25:07,394 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-06-21 09:25:07,394 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-06-21 09:25:07,441 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 19.8 KiB, free 413.3 MiB)\n",
      "2022-06-21 09:25:07,443 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 413.3 MiB)\n",
      "2022-06-21 09:25:07,443 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 (size: 9.4 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:07,444 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-21 09:25:07,444 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-21 09:25:07,445 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "2022-06-21 09:25:07,446 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.200.156.170, executor 2, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()\n",
      "2022-06-21 09:25:07,647 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.200.156.170:35961 (size: 9.4 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:09,796 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.200.156.170:35961 (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:11,946 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4501 ms on 10.200.156.170 (executor 2) (1/1)\n",
      "2022-06-21 09:25:11,946 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2022-06-21 09:25:11,947 INFO scheduler.DAGScheduler: ResultStage 1 (load at NativeMethodAccessorImpl.java:0) finished in 4.552 s\n",
      "2022-06-21 09:25:11,947 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-21 09:25:11,947 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "2022-06-21 09:25:11,949 INFO scheduler.DAGScheduler: Job 1 finished: load at NativeMethodAccessorImpl.java:0, took 4.556871 s\n",
      "type of data frame <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "2022-06-21 09:25:12,030 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-21 09:25:12,030 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-21 09:25:12,030 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Datetime: string, Open: double, High: double, Low: double, Close: double ... 5 more fields>\n",
      "2022-06-21 09:25:12,094 INFO codegen.CodeGenerator: Code generated in 23.494335 ms\n",
      "2022-06-21 09:25:12,099 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 250.2 KiB, free 413.1 MiB)\n",
      "2022-06-21 09:25:12,138 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 413.0 MiB)\n",
      "2022-06-21 09:25:12,139 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:12,140 INFO spark.SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0\n",
      "2022-06-21 09:25:12,143 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-21 09:25:12,158 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2022-06-21 09:25:12,159 INFO scheduler.DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-06-21 09:25:12,159 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2022-06-21 09:25:12,159 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-06-21 09:25:12,159 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-06-21 09:25:12,160 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-06-21 09:25:12,164 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.2 KiB, free 413.0 MiB)\n",
      "2022-06-21 09:25:12,167 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 413.0 MiB)\n",
      "2022-06-21 09:25:12,168 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 (size: 7.0 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:12,168 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-21 09:25:12,168 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-21 09:25:12,168 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "2022-06-21 09:25:12,170 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.200.156.170, executor 2, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()\n",
      "2022-06-21 09:25:12,200 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.200.156.170:35961 (size: 7.0 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:12,294 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.200.156.170:35961 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:12,725 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 555 ms on 10.200.156.170 (executor 2) (1/1)\n",
      "2022-06-21 09:25:12,725 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2022-06-21 09:25:12,726 INFO scheduler.DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.565 s\n",
      "2022-06-21 09:25:12,726 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-21 09:25:12,726 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "2022-06-21 09:25:12,726 INFO scheduler.DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.568004 s\n",
      "2022-06-21 09:25:12,759 INFO codegen.CodeGenerator: Code generated in 20.324128 ms\n",
      "+-------------------+------------------+------------------+------------------+------------------+------+------+\n",
      "|           Datetime|              Open|              High|               Low|             Close|Volume|ticker|\n",
      "+-------------------+------------------+------------------+------------------+------------------+------+------+\n",
      "|2022-06-14 09:30:00|115.98999786376953|116.69999694824219|115.98999786376953|116.27999877929688| 16358|     A|\n",
      "|2022-06-14 09:35:00|116.01000213623047|116.41000366210938|115.69999694824219|115.69999694824219|  9175|     A|\n",
      "|2022-06-14 09:40:00|115.23999786376953|115.23999786376953|114.75700378417969|114.77999877929688|  2321|     A|\n",
      "|2022-06-14 09:45:00|114.87999725341797|115.02999877929688|114.77999877929688|114.91000366210938| 10372|     A|\n",
      "|2022-06-14 09:50:00|114.76000213623047|114.93000030517578|114.72000122070312|114.93000030517578|  5237|     A|\n",
      "|2022-06-14 09:55:00|115.37000274658203|115.66000366210938|115.37000274658203|115.66000366210938|  4587|     A|\n",
      "|2022-06-14 10:00:00|115.66999816894531|115.83000183105469|            115.25| 115.3499984741211| 45640|     A|\n",
      "|2022-06-14 10:05:00|            115.25|115.33999633789062|114.66999816894531|114.74800109863281|  7391|     A|\n",
      "|2022-06-14 10:10:00|114.72000122070312|114.72000122070312|114.39199829101562|114.43000030517578|  7931|     A|\n",
      "|2022-06-14 10:15:00|114.30000305175781| 114.5199966430664|114.12999725341797| 114.3499984741211|  7063|     A|\n",
      "|2022-06-14 10:20:00|114.05999755859375|114.24500274658203|114.01499938964844|114.23999786376953|  6444|     A|\n",
      "|2022-06-14 10:25:00| 114.1500015258789|114.19000244140625|113.83000183105469|113.83000183105469|  9951|     A|\n",
      "|2022-06-14 10:30:00| 113.7300033569336|114.21499633789062| 113.7300033569336|114.13999938964844| 10249|     A|\n",
      "|2022-06-14 10:35:00| 114.1500015258789| 114.3499984741211|113.91000366210938|114.00499725341797| 26553|     A|\n",
      "|2022-06-14 10:40:00|113.94999694824219|113.94999694824219|            113.25|113.30999755859375| 43415|     A|\n",
      "|2022-06-14 10:45:00|113.31999969482422| 114.2249984741211|            113.25| 114.2249984741211| 52354|     A|\n",
      "|2022-06-14 10:50:00| 114.2300033569336|114.52999877929688|114.16000366210938|114.20999908447266|  9172|     A|\n",
      "|2022-06-14 10:55:00|114.12999725341797|114.16000366210938|113.81999969482422| 113.9000015258789| 14047|     A|\n",
      "|2022-06-14 11:00:00|113.91999816894531|114.28060150146484|113.87999725341797|113.98999786376953| 19742|     A|\n",
      "|2022-06-14 11:05:00|113.87999725341797|113.87999725341797|113.62999725341797|113.62999725341797|  5836|     A|\n",
      "+-------------------+------------------+------------------+------------------+------------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "2022-06-21 09:25:12,771 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-21 09:25:12,771 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-21 09:25:12,771 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Datetime: string, Open: double, High: double, Low: double, Close: double ... 5 more fields>\n",
      "2022-06-21 09:25:12,776 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 250.2 KiB, free 412.8 MiB)\n",
      "2022-06-21 09:25:12,794 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 412.7 MiB)\n",
      "2022-06-21 09:25:12,794 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.7 MiB)\n",
      "2022-06-21 09:25:12,797 INFO spark.SparkContext: Created broadcast 6 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2022-06-21 09:25:12,798 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-21 09:25:12,927 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 in memory (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:12,932 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.200.156.170:35961 in memory (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:12,937 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 in memory (size: 9.4 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:12,939 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.200.156.170:35961 in memory (size: 9.4 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:12,943 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 in memory (size: 7.0 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:12,944 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.200.156.170:35961 in memory (size: 7.0 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:13,294 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-21 09:25:13,294 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-21 09:25:13,295 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Datetime: string, Open: double, High: double, Low: double, Close: double ... 5 more fields>\n",
      "2022-06-21 09:25:13,365 INFO codegen.CodeGenerator: Code generated in 15.718735 ms\n",
      "2022-06-21 09:25:13,370 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 250.2 KiB, free 412.8 MiB)\n",
      "2022-06-21 09:25:13,385 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 412.8 MiB)\n",
      "2022-06-21 09:25:13,386 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:13,386 INFO spark.SparkContext: Created broadcast 7 from summary at NativeMethodAccessorImpl.java:0\n",
      "2022-06-21 09:25:13,387 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-21 09:25:13,497 INFO spark.SparkContext: Starting job: summary at NativeMethodAccessorImpl.java:0\n",
      "2022-06-21 09:25:13,509 INFO scheduler.DAGScheduler: Registering RDD 23 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "2022-06-21 09:25:13,512 INFO scheduler.DAGScheduler: Got job 3 (summary at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-06-21 09:25:13,512 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2022-06-21 09:25:13,512 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "2022-06-21 09:25:13,513 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)\n",
      "2022-06-21 09:25:13,514 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[23] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-06-21 09:25:13,533 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 42.8 KiB, free 412.7 MiB)\n",
      "2022-06-21 09:25:13,535 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 412.7 MiB)\n",
      "2022-06-21 09:25:13,535 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 (size: 17.3 KiB, free: 413.7 MiB)\n",
      "2022-06-21 09:25:13,536 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-21 09:25:13,537 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[23] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-21 09:25:13,537 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "2022-06-21 09:25:13,539 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.200.156.169, executor 1, partition 0, PROCESS_LOCAL, 4889 bytes) taskResourceAssignments Map()\n",
      "2022-06-21 09:25:13,570 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.200.156.169:45245 (size: 17.3 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:13,784 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.200.156.169:45245 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:15,203 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1665 ms on 10.200.156.169 (executor 1) (1/1)\n",
      "2022-06-21 09:25:15,203 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2022-06-21 09:25:15,205 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (summary at NativeMethodAccessorImpl.java:0) finished in 1.688 s\n",
      "2022-06-21 09:25:15,205 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2022-06-21 09:25:15,206 INFO scheduler.DAGScheduler: running: Set()\n",
      "2022-06-21 09:25:15,206 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)\n",
      "2022-06-21 09:25:15,206 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2022-06-21 09:25:15,209 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[26] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-06-21 09:25:15,220 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 58.2 KiB, free 412.7 MiB)\n",
      "2022-06-21 09:25:15,222 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 21.7 KiB, free 412.6 MiB)\n",
      "2022-06-21 09:25:15,223 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 (size: 21.7 KiB, free: 413.7 MiB)\n",
      "2022-06-21 09:25:15,223 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-21 09:25:15,224 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[26] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-21 09:25:15,224 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "2022-06-21 09:25:15,229 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.200.156.169, executor 1, partition 0, NODE_LOCAL, 4472 bytes) taskResourceAssignments Map()\n",
      "2022-06-21 09:25:15,252 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.200.156.169:45245 (size: 21.7 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:15,293 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.200.156.169:48188\n",
      "2022-06-21 09:25:15,732 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 504 ms on 10.200.156.169 (executor 1) (1/1)\n",
      "2022-06-21 09:25:15,733 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2022-06-21 09:25:15,734 INFO scheduler.DAGScheduler: ResultStage 4 (summary at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "2022-06-21 09:25:15,734 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-21 09:25:15,734 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "2022-06-21 09:25:15,735 INFO scheduler.DAGScheduler: Job 3 finished: summary at NativeMethodAccessorImpl.java:0, took 2.237946 s\n",
      "2022-06-21 09:25:15,801 INFO codegen.CodeGenerator: Code generated in 15.618959 ms\n",
      "2022-06-21 09:25:16,191 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 in memory (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:16,194 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.200.156.169:45245 in memory (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:16,227 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 in memory (size: 17.3 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:16,227 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.200.156.169:45245 in memory (size: 17.3 KiB, free: 413.9 MiB)\n",
      "2022-06-21 09:25:16,236 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on stocks-ingest-e28ee1b6-54957e818593f515-driver-svc.default-tenant.svc:7079 in memory (size: 21.7 KiB, free: 413.8 MiB)\n",
      "2022-06-21 09:25:16,238 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.200.156.169:45245 in memory (size: 21.7 KiB, free: 413.9 MiB)\n"
     ]
    }
   ],
   "source": [
    "my_func = code_to_function(\"func\", kind=\"spark\")\n",
    "\n",
    "my_func.with_driver_requests(cpu=1, mem=\"1G\")\n",
    "my_func.with_executor_requests(cpu=1, mem=\"1G\")\n",
    "my_func.with_igz_spark()\n",
    "\n",
    "# Enables using the default image (can be replace with specifying a specific image with .spec.image)\n",
    "my_func.spec.use_default_image = True\n",
    "\n",
    "# Not a must - default: 1\n",
    "my_func.spec.replicas = 2\n",
    "\n",
    "# If needed, sparkConf can be modified like this:\n",
    "# my_func.spec.spark_conf['spark.specific.config.key'] = 'value'\n",
    "\n",
    "config = fstore.RunConfig(local=False, function=my_func, handler=\"ingest_handler\")\n",
    "quotes_df = fstore.ingest(feature_set, source, run_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"stocks.*\",\n",
    "]\n",
    "\n",
    "vector = fstore.FeatureVector(\"stocks-vec\",features=features,description=\"this is my vector\",with_indexes=True)\n",
    "resp = fstore.get_offline_features(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = resp.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03297424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f843fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
