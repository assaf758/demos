{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1be8c639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-06-19 14:38:11,494 [info] loaded project stocks from MLRun DB\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "project = mlrun.get_or_create_project(name='stocks',user_project=True, context=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6acec971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlrun.runtimes import Spark3Runtime\n",
    "# Spark3Runtime.deploy_default_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "078ae865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun: start-code\n",
    "from mlrun.feature_store.api import ingest\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def ingest_handler(context):\n",
    "    ingest(mlrun_context=context) # The handler function must call ingest with the mlrun_context\n",
    "\n",
    "# You can add your own PySpark code as a graph step:\n",
    "def filter_func(df, context=None):\n",
    "    return df.filter(\"bid>55\") # PySpark code\n",
    "\n",
    "\n",
    "def add_column(df, context=None):\n",
    "    return_df = df.withColumn(\"bid_ask_diff\", df.bid-df.ask)\n",
    "    return return_df\n",
    "\n",
    "def print_dataframe(df, contest=None):\n",
    "    print(\"type of data frame {}\".format(type(df)))        \n",
    "    print(df.show())\n",
    "    return df\n",
    "\n",
    "# mlrun: end-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cee14cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.serving.states.TaskStep at 0x7efce6361d10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun.datastore.sources import CSVSource\n",
    "from mlrun import code_to_function\n",
    "import mlrun.feature_store as fstore\n",
    "\n",
    "feature_set = fstore.FeatureSet(\"stocks\", entities=[fstore.Entity(\"ticker\")],timestamp_key='Datetime', engine=\"spark\")\n",
    "\n",
    "#source = CSVSource(\"mycsv\", path=\"v3io:///users/aviaIguazio/demos/stock-analysis/data/input.csv\")\n",
    "source = CSVSource(\"mycsv\", path=\"v3io:///projects/input.csv\")\n",
    "\n",
    "feature_set.graph\\\n",
    "    .to(name=\"print_dataframe\", handler=\"print_dataframe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8b59aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"396pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 396.48 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-94 392.4837,-94 392.4837,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"38.5476,-27.0493 40.698,-27.1479 42.8263,-27.2953 44.9236,-27.4913 46.9815,-27.7353 48.9917,-28.0266 50.9463,-28.3645 52.8377,-28.7479 54.6587,-29.1759 56.4025,-29.6472 58.0628,-30.1606 59.634,-30.7147 61.1107,-31.308 62.4882,-31.9388 63.7625,-32.6054 64.9302,-33.3059 65.9882,-34.0385 66.9343,-34.8012 67.7669,-35.5918 68.4849,-36.4082 69.0878,-37.2481 69.5758,-38.1093 69.9496,-38.9894 70.2102,-39.886 70.3595,-40.7965 70.3997,-41.7186 70.3334,-42.6497 70.1636,-43.5873 69.8937,-44.5287 69.5276,-45.4713 69.0691,-46.4127 68.5225,-47.3503 67.8923,-48.2814 67.1831,-49.2035 66.3996,-50.114 65.5464,-51.0106 64.6285,-51.8907 63.6504,-52.7519 62.617,-53.5918 61.5329,-54.4082 60.4024,-55.1988 59.2299,-55.9615 58.0197,-56.6941 56.7755,-57.3946 55.5012,-58.0612 54.2002,-58.692 52.8757,-59.2853 51.5309,-59.8394 50.1684,-60.3528 48.7908,-60.8241 47.4003,-61.2521 45.9989,-61.6355 44.5886,-61.9734 43.1708,-62.2647 41.7472,-62.5087 40.3189,-62.7047 38.8872,-62.8521 37.4531,-62.9507 36.0175,-63 34.5815,-63 33.146,-62.9507 31.7119,-62.8521 30.2801,-62.7047 28.8519,-62.5087 27.4282,-62.2647 26.0105,-61.9734 24.6001,-61.6355 23.1988,-61.2521 21.8083,-60.8241 20.4306,-60.3528 19.0681,-59.8394 17.7233,-59.2853 16.3989,-58.692 15.0979,-58.0612 13.8236,-57.3946 12.5794,-56.6941 11.3691,-55.9615 10.1967,-55.1988 9.0662,-54.4082 7.982,-53.5918 6.9486,-52.7519 5.9706,-51.8907 5.0526,-51.0106 4.1995,-50.114 3.4159,-49.2035 2.7067,-48.2814 2.0765,-47.3503 1.53,-46.4127 1.0715,-45.4713 .7053,-44.5287 .4355,-43.5873 .2657,-42.6497 .1993,-41.7186 .2395,-40.7965 .3888,-39.886 .6495,-38.9894 1.0232,-38.1093 1.5112,-37.2481 2.1141,-36.4082 2.8321,-35.5918 3.6647,-34.8012 4.6109,-34.0385 5.6689,-33.3059 6.8365,-32.6054 8.1108,-31.9388 9.4884,-31.308 10.9651,-30.7147 12.5362,-30.1606 14.1966,-29.6472 15.9404,-29.1759 17.7614,-28.7479 19.6528,-28.3645 21.6074,-28.0266 23.6176,-27.7353 25.6755,-27.4913 27.7728,-27.2953 29.901,-27.1479 32.0515,-27.0493 34.2154,-27 36.3837,-27 38.5476,-27.0493\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.2995\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">start</text>\n",
       "</g>\n",
       "<!-- print_dataframe -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>print_dataframe</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"193.0414\" cy=\"-45\" rx=\"86.3847\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.0414\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">print_dataframe</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;print_dataframe -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;print_dataframe</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M69.9166,-45C77.9754,-45 86.9455,-45 96.268,-45\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.4956,-48.5001 106.4956,-45 96.4955,-41.5001 96.4956,-48.5001\"/>\n",
       "</g>\n",
       "<!-- parquet -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>parquet</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M388.4837,-86.7273C388.4837,-88.5331 372.1238,-90 351.9837,-90 331.8436,-90 315.4837,-88.5331 315.4837,-86.7273 315.4837,-86.7273 315.4837,-57.2727 315.4837,-57.2727 315.4837,-55.4669 331.8436,-54 351.9837,-54 372.1238,-54 388.4837,-55.4669 388.4837,-57.2727 388.4837,-57.2727 388.4837,-86.7273 388.4837,-86.7273\"/>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M388.4837,-86.7273C388.4837,-84.9214 372.1238,-83.4545 351.9837,-83.4545 331.8436,-83.4545 315.4837,-84.9214 315.4837,-86.7273\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.9837\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">parquet</text>\n",
       "</g>\n",
       "<!-- print_dataframe&#45;&gt;parquet -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>print_dataframe&#45;&gt;parquet</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M260.4472,-56.4504C275.5618,-59.018 291.2725,-61.6868 305.3234,-64.0737\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"304.7841,-67.5321 315.229,-65.7564 305.9564,-60.631 304.7841,-67.5321\"/>\n",
       "</g>\n",
       "<!-- nosql -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>nosql</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M379.4837,-32.7273C379.4837,-34.5331 367.1578,-36 351.9837,-36 336.8097,-36 324.4837,-34.5331 324.4837,-32.7273 324.4837,-32.7273 324.4837,-3.2727 324.4837,-3.2727 324.4837,-1.4669 336.8097,0 351.9837,0 367.1578,0 379.4837,-1.4669 379.4837,-3.2727 379.4837,-3.2727 379.4837,-32.7273 379.4837,-32.7273\"/>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M379.4837,-32.7273C379.4837,-30.9214 367.1578,-29.4545 351.9837,-29.4545 336.8097,-29.4545 324.4837,-30.9214 324.4837,-32.7273\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.9837\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nosql</text>\n",
       "</g>\n",
       "<!-- print_dataframe&#45;&gt;nosql -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>print_dataframe&#45;&gt;nosql</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M260.4472,-33.5496C278.9092,-30.4134 298.2605,-27.1261 314.39,-24.3862\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"315.2098,-27.7971 324.4824,-22.6717 314.0374,-20.896 315.2098,-27.7971\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7efce6369450>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "# Setting default targets (nosql & parquet)\n",
    "feature_set.set_targets(with_defaults=True) \n",
    "feature_set.plot(rankdir=\"LR\", with_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f31ad9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-06-19 14:38:23,867 [info] starting run stocks-ingest uid=0ad6d9e0a5034930a12ab6b267d84db0 DB=http://mlrun-api:8080\n",
      "++ id -u\n",
      "+ myuid=1000\n",
      "++ id -g\n",
      "+ mygid=1000\n",
      "+ set +e\n",
      "++ getent passwd 1000\n",
      "+ uidentry=iguazio:x:1000:1000::/igz:/bin/bash\n",
      "+ set -e\n",
      "+ '[' -z iguazio:x:1000:1000::/igz:/bin/bash ']'\n",
      "+ SPARK_CLASSPATH=':/spark/jars/*'\n",
      "+ env\n",
      "+ grep SPARK_JAVA_OPT_\n",
      "+ sort -t_ -k4 -n\n",
      "+ sed 's/[^=]*=\\(.*\\)/\\1/g'\n",
      "+ readarray -t SPARK_EXECUTOR_JAVA_OPTS\n",
      "+ '[' -n '' ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -n /hadoop ']'\n",
      "+ '[' -z '' ']'\n",
      "++ /hadoop/bin/hadoop classpath\n",
      "+ export 'SPARK_DIST_CLASSPATH=/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/lib/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.2.0.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar'\n",
      "+ SPARK_DIST_CLASSPATH='/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/lib/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.2.0.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/opt/spark/conf:/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ case \"$1\" in\n",
      "+ shift 1\n",
      "+ CMD=(\"$SPARK_HOME/bin/spark-submit\" --conf \"spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS\" --deploy-mode client \"$@\")\n",
      "+ exec /usr/bin/tini -s -- /spark/bin/spark-submit --conf spark.driver.bindAddress=10.200.156.146 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner local:///etc/config/mlrun/spark-function-code.py\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2022-06-19 14:38:32,397 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "> 2022-06-19 14:38:39,173 [info] starting ingestion task to store://feature-sets/stocks-avia/stocks:latest.\n",
      "2022-06-19 14:38:39,775 INFO spark.SparkContext: Running Spark version 3.1.2\n",
      "2022-06-19 14:38:39,814 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-06-19 14:38:39,826 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2022-06-19 14:38:39,827 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-06-19 14:38:39,827 INFO spark.SparkContext: Submitted application: stocks-ingest-0ad6d9e0a5034930a12ab6b267d84db0\n",
      "2022-06-19 14:38:39,848 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2022-06-19 14:38:39,866 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2022-06-19 14:38:39,868 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2022-06-19 14:38:39,928 INFO spark.SecurityManager: Changing view acls to: iguazio\n",
      "2022-06-19 14:38:39,928 INFO spark.SecurityManager: Changing modify acls to: iguazio\n",
      "2022-06-19 14:38:39,928 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2022-06-19 14:38:39,928 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2022-06-19 14:38:39,929 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(iguazio); groups with view permissions: Set(); users  with modify permissions: Set(iguazio); groups with modify permissions: Set()\n",
      "2022-06-19 14:38:40,206 INFO util.Utils: Successfully started service 'sparkDriver' on port 7078.\n",
      "2022-06-19 14:38:40,230 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2022-06-19 14:38:40,262 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2022-06-19 14:38:40,287 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2022-06-19 14:38:40,287 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2022-06-19 14:38:40,291 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2022-06-19 14:38:40,305 INFO storage.DiskBlockManager: Created local directory at /var/data/spark-d6c2ae3e-23a5-438e-a577-f9aa7a15f85e/blockmgr-18ebbdf4-6051-40cd-a4d4-ccc56c0ae6e8\n",
      "2022-06-19 14:38:40,329 INFO memory.MemoryStore: MemoryStore started with capacity 413.9 MiB\n",
      "2022-06-19 14:38:40,345 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2022-06-19 14:38:40,453 INFO util.log: Logging initialized @10495ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2022-06-19 14:38:40,531 INFO server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-LTS\n",
      "2022-06-19 14:38:40,551 INFO server.Server: Started @10593ms\n",
      "2022-06-19 14:38:40,583 INFO server.AbstractConnector: Started ServerConnector@76e2ad66{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2022-06-19 14:38:40,583 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2022-06-19 14:38:40,605 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6943caaf{/jobs,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,607 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d0ac756{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,608 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32301de1{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,609 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@509c8221{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,609 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@115e0a86{/stages,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,610 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ddad9bd{/stages/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,611 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f015602{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,612 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4112b7c3{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,612 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1370547f{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,613 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c2b7688{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,614 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67a6f8a3{/storage,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,614 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7359338b{/storage/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,615 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7111fad2{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,616 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33f24370{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,617 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7fb67267{/environment,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,620 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3870d8d0{/environment/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,621 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b16675d{/executors,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,622 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bf2f10d{/executors/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,623 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ca2c640{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,628 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15f22381{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,638 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@8b16839{/static,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,639 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a37b4d7{/,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,640 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d818a81{/api,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,641 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3bb867df{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,641 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31d5ef24{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:40,643 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:4040\n",
      "2022-06-19 14:38:40,668 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-hcfs_2.12.jar at file:/spark/v3io-libs/v3io-hcfs_2.12.jar with timestamp 1655649519767\n",
      "2022-06-19 14:38:40,668 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-streaming_2.12.jar at file:/spark/v3io-libs/v3io-spark3-streaming_2.12.jar with timestamp 1655649519767\n",
      "2022-06-19 14:38:40,668 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar at file:/spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar with timestamp 1655649519767\n",
      "2022-06-19 14:38:40,668 INFO spark.SparkContext: Added JAR local:///igz/java/libs/scala-library-2.12.14.jar at file:/igz/java/libs/scala-library-2.12.14.jar with timestamp 1655649519767\n",
      "2022-06-19 14:38:40,668 INFO spark.SparkContext: Added JAR local:///spark/jars/jmx_prometheus_javaagent-0.16.1.jar at file:/spark/jars/jmx_prometheus_javaagent-0.16.1.jar with timestamp 1655649519767\n",
      "2022-06-19 14:38:40,670 WARN spark.SparkContext: File with 'local' scheme local:///igz/java/libs/v3io-pyspark.zip is not supported to add to file server, since it is already available on every node.\n",
      "2022-06-19 14:38:40,790 INFO k8s.SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n",
      "2022-06-19 14:38:41,846 INFO k8s.ExecutorPodsAllocator: Going to request 2 executors from Kubernetes for ResourceProfile Id: 0, target: 2 running: 0.\n",
      "2022-06-19 14:38:42,028 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-06-19 14:38:42,063 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.\n",
      "2022-06-19 14:38:42,063 INFO netty.NettyBlockTransferService: Server created on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079\n",
      "2022-06-19 14:38:42,065 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2022-06-19 14:38:42,075 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-06-19 14:38:42,079 INFO storage.BlockManagerMasterEndpoint: Registering block manager stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 with 413.9 MiB RAM, BlockManagerId(driver, stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-06-19 14:38:42,081 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-06-19 14:38:42,127 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-06-19 14:38:42,158 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-06-19 14:38:42,164 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73888838{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:48,612 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.201.77.167:58570) with ID 2,  ResourceProfileId 0\n",
      "2022-06-19 14:38:48,614 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.201.77.168:42646) with ID 1,  ResourceProfileId 0\n",
      "2022-06-19 14:38:48,649 INFO k8s.KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2022-06-19 14:38:48,678 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.201.77.167:38794 with 413.9 MiB RAM, BlockManagerId(2, 10.201.77.167, 38794, None)\n",
      "2022-06-19 14:38:48,679 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.201.77.168:36811 with 413.9 MiB RAM, BlockManagerId(1, 10.201.77.168, 36811, None)\n",
      "2022-06-19 14:38:48,938 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/spark/spark-warehouse').\n",
      "2022-06-19 14:38:48,938 INFO internal.SharedState: Warehouse path is 'file:/spark/spark-warehouse'.\n",
      "2022-06-19 14:38:48,954 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55bfe31d{/SQL,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:48,955 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3338db09{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:48,956 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5aaced43{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:48,956 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@712f920f{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:48,974 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fb40cb3{/static/sql,null,AVAILABLE,@Spark}\n",
      "2022-06-19 14:38:50,563 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "2022-06-19 14:38:51,549 INFO datasources.InMemoryFileIndex: It took 206 ms to list leaf files for 1 paths.\n",
      "2022-06-19 14:38:51,726 INFO datasources.InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
      "2022-06-19 14:38:53,919 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-19 14:38:53,923 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "2022-06-19 14:38:53,926 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2022-06-19 14:38:54,545 INFO codegen.CodeGenerator: Code generated in 286.579519 ms\n",
      "2022-06-19 14:38:54,605 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 250.3 KiB, free 413.7 MiB)\n",
      "2022-06-19 14:38:54,680 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 413.6 MiB)\n",
      "2022-06-19 14:38:54,682 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:38:54,685 INFO spark.SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0\n",
      "2022-06-19 14:38:54,694 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-19 14:38:54,830 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "2022-06-19 14:38:54,844 INFO scheduler.DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-06-19 14:38:54,845 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "2022-06-19 14:38:54,845 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-06-19 14:38:54,846 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-06-19 14:38:54,850 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-06-19 14:38:54,938 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 413.6 MiB)\n",
      "2022-06-19 14:38:54,941 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 413.6 MiB)\n",
      "2022-06-19 14:38:54,941 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 5.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:38:54,942 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-19 14:38:54,955 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-19 14:38:54,956 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2022-06-19 14:38:54,990 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.201.77.167, executor 2, partition 0, PROCESS_LOCAL, 4869 bytes) taskResourceAssignments Map()\n",
      "2022-06-19 14:38:55,227 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.201.77.167:38794 (size: 5.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:38:56,439 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.201.77.167:38794 (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:38:58,468 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3485 ms on 10.201.77.167 (executor 2) (1/1)\n",
      "2022-06-19 14:38:58,471 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2022-06-19 14:38:58,478 INFO scheduler.DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 3.614 s\n",
      "2022-06-19 14:38:58,482 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-19 14:38:58,483 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "2022-06-19 14:38:58,484 INFO scheduler.DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 3.653767 s\n",
      "2022-06-19 14:38:58,513 INFO codegen.CodeGenerator: Code generated in 13.919705 ms\n",
      "2022-06-19 14:38:58,572 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-19 14:38:58,572 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-19 14:38:58,572 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2022-06-19 14:38:58,582 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 250.3 KiB, free 413.4 MiB)\n",
      "2022-06-19 14:38:58,601 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 413.3 MiB)\n",
      "2022-06-19 14:38:58,601 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:38:58,602 INFO spark.SparkContext: Created broadcast 2 from load at NativeMethodAccessorImpl.java:0\n",
      "2022-06-19 14:38:58,603 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-19 14:38:58,689 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "2022-06-19 14:38:58,690 INFO scheduler.DAGScheduler: Got job 1 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-06-19 14:38:58,690 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (load at NativeMethodAccessorImpl.java:0)\n",
      "2022-06-19 14:38:58,690 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-06-19 14:38:58,691 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-06-19 14:38:58,691 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-06-19 14:38:58,714 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 19.9 KiB, free 413.3 MiB)\n",
      "2022-06-19 14:38:58,750 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 413.3 MiB)\n",
      "2022-06-19 14:38:58,750 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 9.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:38:58,751 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-19 14:38:58,752 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-19 14:38:58,752 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "2022-06-19 14:38:58,753 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.201.77.168, executor 1, partition 0, PROCESS_LOCAL, 4869 bytes) taskResourceAssignments Map()\n",
      "2022-06-19 14:38:58,771 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:38:58,776 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.201.77.167:38794 in memory (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:38:58,784 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 5.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:38:58,785 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.201.77.167:38794 in memory (size: 5.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:38:58,951 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.201.77.168:36811 (size: 9.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:01,106 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.201.77.168:36811 (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:03,252 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4499 ms on 10.201.77.168 (executor 1) (1/1)\n",
      "2022-06-19 14:39:03,252 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2022-06-19 14:39:03,253 INFO scheduler.DAGScheduler: ResultStage 1 (load at NativeMethodAccessorImpl.java:0) finished in 4.560 s\n",
      "2022-06-19 14:39:03,254 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-19 14:39:03,254 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "2022-06-19 14:39:03,255 INFO scheduler.DAGScheduler: Job 1 finished: load at NativeMethodAccessorImpl.java:0, took 4.565989 s\n",
      "type of data frame <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "2022-06-19 14:39:03,331 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-19 14:39:03,332 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-19 14:39:03,332 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Datetime: string, Open: double, High: double, Low: double, Close: double ... 6 more fields>\n",
      "2022-06-19 14:39:03,394 INFO codegen.CodeGenerator: Code generated in 22.371748 ms\n",
      "2022-06-19 14:39:03,399 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 250.2 KiB, free 413.4 MiB)\n",
      "2022-06-19 14:39:03,454 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 9.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:03,454 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 413.3 MiB)\n",
      "2022-06-19 14:39:03,455 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:03,456 INFO spark.SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0\n",
      "2022-06-19 14:39:03,458 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.201.77.168:36811 in memory (size: 9.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:03,459 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-19 14:39:03,472 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2022-06-19 14:39:03,473 INFO scheduler.DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-06-19 14:39:03,473 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2022-06-19 14:39:03,473 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-06-19 14:39:03,473 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-06-19 14:39:03,474 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-06-19 14:39:03,478 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.6 KiB, free 413.3 MiB)\n",
      "2022-06-19 14:39:03,480 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 413.3 MiB)\n",
      "2022-06-19 14:39:03,480 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 7.1 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:03,481 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-19 14:39:03,481 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-19 14:39:03,481 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "2022-06-19 14:39:03,482 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.201.77.167, executor 2, partition 0, PROCESS_LOCAL, 4869 bytes) taskResourceAssignments Map()\n",
      "2022-06-19 14:39:03,513 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.201.77.167:38794 (size: 7.1 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:03,600 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.201.77.167:38794 (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:03,947 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 465 ms on 10.201.77.167 (executor 2) (1/1)\n",
      "2022-06-19 14:39:03,948 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2022-06-19 14:39:03,948 INFO scheduler.DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.473 s\n",
      "2022-06-19 14:39:03,949 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-19 14:39:03,949 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "2022-06-19 14:39:03,949 INFO scheduler.DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.476564 s\n",
      "2022-06-19 14:39:03,980 INFO codegen.CodeGenerator: Code generated in 20.019085 ms\n",
      "+-------------------+------------------+------------------+------------------+------------------+------+------+-------------+\n",
      "|           Datetime|              Open|              High|               Low|             Close|Volume|ticker|ticker2onehot|\n",
      "+-------------------+------------------+------------------+------------------+------------------+------+------+-------------+\n",
      "|2022-06-06 09:30:00| 129.3800048828125|             130.0| 129.1699981689453| 129.2550048828125| 61282|     A|            A|\n",
      "|2022-06-06 09:35:00|129.18150329589844|129.39999389648438| 129.0399932861328|129.39999389648438| 17839|     A|            A|\n",
      "|2022-06-06 09:40:00|129.42999267578125|129.88999938964844| 129.3000030517578|129.39999389648438|  6999|     A|            A|\n",
      "|2022-06-06 09:45:00|129.50999450683594|129.50999450683594| 128.8800048828125| 128.8800048828125| 20361|     A|            A|\n",
      "|2022-06-06 09:50:00| 128.7949981689453|129.00999450683594|128.66000366210938|128.99000549316406|  9078|     A|            A|\n",
      "|2022-06-06 09:55:00|128.89999389648438|129.25999450683594|128.89999389648438|129.25999450683594|  7042|     A|            A|\n",
      "|2022-06-06 10:00:00|129.38499450683594|129.91000366210938|129.38499450683594|129.72000122070312|  8425|     A|            A|\n",
      "|2022-06-06 10:05:00|129.77000427246094|129.97999572753906|129.57000732421875|129.88999938964844| 10900|     A|            A|\n",
      "|2022-06-06 10:10:00| 129.8300018310547|129.91250610351562| 129.5850067138672| 129.5850067138672|  8561|     A|            A|\n",
      "|2022-06-06 10:15:00|129.63999938964844| 129.7949981689453| 129.3350067138672| 129.3350067138672|  6900|     A|            A|\n",
      "|2022-06-06 10:20:00|129.38999938964844|129.72999572753906| 129.2899932861328|129.61500549316406| 14399|     A|            A|\n",
      "|2022-06-06 10:25:00|129.60000610351562|129.64999389648438|129.44000244140625| 129.4499969482422|  6692|     A|            A|\n",
      "|2022-06-06 10:30:00| 129.5050048828125|           129.625| 129.3699951171875|129.52000427246094| 78482|     A|            A|\n",
      "|2022-06-06 10:35:00| 129.5500030517578| 129.5500030517578|129.22000122070312|            129.25| 26094|     A|            A|\n",
      "|2022-06-06 10:40:00|129.38999938964844|129.42999267578125|129.07000732421875|129.11000061035156|  6696|     A|            A|\n",
      "|2022-06-06 10:45:00| 129.1999969482422| 129.3000030517578|129.01499938964844| 129.1999969482422| 11653|     A|            A|\n",
      "|2022-06-06 10:50:00|129.19000244140625| 129.2899932861328| 129.1199951171875|129.14500427246094|  7750|     A|            A|\n",
      "|2022-06-06 10:55:00| 129.1999969482422| 129.4600067138672|129.05999755859375|129.40240478515625|  8930|     A|            A|\n",
      "|2022-06-06 11:00:00|129.47999572753906|129.56500244140625|129.39999389648438|129.44000244140625|  9339|     A|            A|\n",
      "|2022-06-06 11:05:00|129.47999572753906| 129.5800018310547| 129.1999969482422|129.22000122070312|  9048|     A|            A|\n",
      "+-------------------+------------------+------------------+------------------+------------------+------+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "2022-06-19 14:39:03,992 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-19 14:39:03,992 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-19 14:39:03,992 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Datetime: string, Open: double, High: double, Low: double, Close: double ... 6 more fields>\n",
      "2022-06-19 14:39:03,997 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 250.2 KiB, free 413.1 MiB)\n",
      "2022-06-19 14:39:04,013 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 413.0 MiB)\n",
      "2022-06-19 14:39:04,014 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:04,014 INFO spark.SparkContext: Created broadcast 6 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2022-06-19 14:39:04,015 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-19 14:39:04,382 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:04,385 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.201.77.167:38794 in memory (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:04,427 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.201.77.167:38794 in memory (size: 7.1 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:04,427 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 7.1 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:04,495 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-19 14:39:04,496 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-19 14:39:04,496 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Datetime: string, Open: double, High: double, Low: double, Close: double ... 6 more fields>\n",
      "2022-06-19 14:39:04,568 INFO codegen.CodeGenerator: Code generated in 16.033199 ms\n",
      "2022-06-19 14:39:04,573 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 250.2 KiB, free 413.1 MiB)\n",
      "2022-06-19 14:39:04,586 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 413.1 MiB)\n",
      "2022-06-19 14:39:04,587 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:04,588 INFO spark.SparkContext: Created broadcast 7 from summary at NativeMethodAccessorImpl.java:0\n",
      "2022-06-19 14:39:04,588 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-19 14:39:04,698 INFO spark.SparkContext: Starting job: summary at NativeMethodAccessorImpl.java:0\n",
      "2022-06-19 14:39:04,700 INFO scheduler.DAGScheduler: Registering RDD 23 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "2022-06-19 14:39:04,702 INFO scheduler.DAGScheduler: Got job 3 (summary at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-06-19 14:39:04,702 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2022-06-19 14:39:04,703 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "2022-06-19 14:39:04,703 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)\n",
      "2022-06-19 14:39:04,705 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[23] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-06-19 14:39:04,733 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 45.5 KiB, free 413.0 MiB)\n",
      "2022-06-19 14:39:04,735 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 18.1 KiB, free 413.0 MiB)\n",
      "2022-06-19 14:39:04,736 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 18.1 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:04,736 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-19 14:39:04,737 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[23] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-19 14:39:04,737 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "2022-06-19 14:39:04,739 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.201.77.167, executor 2, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
      "2022-06-19 14:39:04,755 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.201.77.167:38794 (size: 18.1 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:04,948 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.201.77.167:38794 (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:06,526 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1788 ms on 10.201.77.167 (executor 2) (1/1)\n",
      "2022-06-19 14:39:06,526 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2022-06-19 14:39:06,528 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (summary at NativeMethodAccessorImpl.java:0) finished in 1.820 s\n",
      "2022-06-19 14:39:06,529 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2022-06-19 14:39:06,529 INFO scheduler.DAGScheduler: running: Set()\n",
      "2022-06-19 14:39:06,529 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)\n",
      "2022-06-19 14:39:06,530 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2022-06-19 14:39:06,532 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[26] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-06-19 14:39:06,543 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 62.4 KiB, free 412.9 MiB)\n",
      "2022-06-19 14:39:06,545 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 22.8 KiB, free 412.9 MiB)\n",
      "2022-06-19 14:39:06,546 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 22.8 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:06,546 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-19 14:39:06,547 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[26] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-19 14:39:06,547 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "2022-06-19 14:39:06,552 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.201.77.167, executor 2, partition 0, NODE_LOCAL, 4472 bytes) taskResourceAssignments Map()\n",
      "2022-06-19 14:39:06,566 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.201.77.167:38794 (size: 22.8 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:06,602 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.201.77.167:58570\n",
      "2022-06-19 14:39:06,998 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 448 ms on 10.201.77.167 (executor 2) (1/1)\n",
      "2022-06-19 14:39:06,999 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2022-06-19 14:39:06,999 INFO scheduler.DAGScheduler: ResultStage 4 (summary at NativeMethodAccessorImpl.java:0) finished in 0.460 s\n",
      "2022-06-19 14:39:07,000 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-19 14:39:07,000 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "2022-06-19 14:39:07,000 INFO scheduler.DAGScheduler: Job 3 finished: summary at NativeMethodAccessorImpl.java:0, took 2.302228 s\n",
      "2022-06-19 14:39:07,040 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 18.1 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:07,041 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.201.77.167:38794 in memory (size: 18.1 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:07,045 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 22.8 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:07,047 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.201.77.167:38794 in memory (size: 22.8 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:07,051 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:07,052 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.201.77.167:38794 in memory (size: 44.4 KiB, free: 413.9 MiB)\n",
      "2022-06-19 14:39:07,072 INFO codegen.CodeGenerator: Code generated in 16.065022 ms\n",
      "2022-06-19 14:39:10,527 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-19 14:39:10,528 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-19 14:39:10,528 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Open: double, High: double, Low: double, Close: double, Volume: int ... 3 more fields>\n",
      "2022-06-19 14:39:10,556 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "2022-06-19 14:39:10,986 INFO codegen.CodeGenerator: Code generated in 248.314056 ms\n",
      "2022-06-19 14:39:11,586 INFO codegen.CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$ is 16948 bytes\n",
      "2022-06-19 14:39:11,587 INFO codegen.CodeGenerator: Code generated in 332.417882 ms\n",
      "2022-06-19 14:39:11,592 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 250.2 KiB, free 413.1 MiB)\n",
      "2022-06-19 14:39:11,606 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 413.1 MiB)\n",
      "2022-06-19 14:39:11,607 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:11,607 INFO spark.SparkContext: Created broadcast 10 from toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88\n",
      "2022-06-19 14:39:11,608 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-19 14:39:11,644 INFO spark.SparkContext: Starting job: toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88\n",
      "2022-06-19 14:39:11,645 INFO scheduler.DAGScheduler: Registering RDD 30 (toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88) as input to shuffle 1\n",
      "2022-06-19 14:39:11,645 INFO scheduler.DAGScheduler: Got job 4 (toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88) with 1 output partitions\n",
      "2022-06-19 14:39:11,645 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88)\n",
      "2022-06-19 14:39:11,645 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "2022-06-19 14:39:11,645 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 5)\n",
      "2022-06-19 14:39:11,646 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[30] at toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88), which has no missing parents\n",
      "2022-06-19 14:39:11,654 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 219.1 KiB, free 412.8 MiB)\n",
      "2022-06-19 14:39:11,656 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 53.3 KiB, free 412.8 MiB)\n",
      "2022-06-19 14:39:11,657 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 53.3 KiB, free: 413.7 MiB)\n",
      "2022-06-19 14:39:11,657 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-19 14:39:11,657 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[30] at toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-19 14:39:11,658 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "2022-06-19 14:39:11,659 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.201.77.168, executor 1, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
      "2022-06-19 14:39:11,703 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.201.77.168:36811 (size: 53.3 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:12,447 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.201.77.168:36811 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:12,852 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1194 ms on 10.201.77.168 (executor 1) (1/1)\n",
      "2022-06-19 14:39:12,852 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2022-06-19 14:39:12,853 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88) finished in 1.206 s\n",
      "2022-06-19 14:39:12,853 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2022-06-19 14:39:12,853 INFO scheduler.DAGScheduler: running: Set()\n",
      "2022-06-19 14:39:12,853 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 6)\n",
      "2022-06-19 14:39:12,853 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2022-06-19 14:39:12,854 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[33] at toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88), which has no missing parents\n",
      "2022-06-19 14:39:12,857 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 155.5 KiB, free 412.6 MiB)\n",
      "2022-06-19 14:39:12,861 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 38.9 KiB, free 412.6 MiB)\n",
      "2022-06-19 14:39:12,862 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 38.9 KiB, free: 413.7 MiB)\n",
      "2022-06-19 14:39:12,862 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-19 14:39:12,862 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[33] at toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-19 14:39:12,863 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "2022-06-19 14:39:12,864 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (10.201.77.168, executor 1, partition 0, NODE_LOCAL, 4472 bytes) taskResourceAssignments Map()\n",
      "2022-06-19 14:39:12,879 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.201.77.168:36811 (size: 38.9 KiB, free: 413.7 MiB)\n",
      "2022-06-19 14:39:12,911 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.201.77.168:42646\n",
      "2022-06-19 14:39:13,197 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 334 ms on 10.201.77.168 (executor 1) (1/1)\n",
      "2022-06-19 14:39:13,197 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "2022-06-19 14:39:13,198 INFO scheduler.DAGScheduler: ResultStage 6 (toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88) finished in 0.343 s\n",
      "2022-06-19 14:39:13,198 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-19 14:39:13,198 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "2022-06-19 14:39:13,198 INFO scheduler.DAGScheduler: Job 4 finished: toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:88, took 1.554541 s\n",
      "2022-06-19 14:39:13,295 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 53.3 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:13,299 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.201.77.168:36811 in memory (size: 53.3 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:13,328 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 38.9 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:13,329 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.201.77.168:36811 in memory (size: 38.9 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:13,332 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-19 14:39:13,332 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-19 14:39:13,333 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Datetime: string, Open: double, High: double, Low: double, Close: double ... 6 more fields>\n",
      "2022-06-19 14:39:13,339 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 250.2 KiB, free 412.8 MiB)\n",
      "2022-06-19 14:39:13,352 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 412.8 MiB)\n",
      "2022-06-19 14:39:13,353 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:13,353 INFO spark.SparkContext: Created broadcast 13 from toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:51\n",
      "2022-06-19 14:39:13,354 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-19 14:39:13,360 INFO spark.SparkContext: Starting job: toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:51\n",
      "2022-06-19 14:39:13,361 INFO scheduler.DAGScheduler: Got job 5 (toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:51) with 1 output partitions\n",
      "2022-06-19 14:39:13,361 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:51)\n",
      "2022-06-19 14:39:13,361 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-06-19 14:39:13,362 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-06-19 14:39:13,368 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[36] at toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:51), which has no missing parents\n",
      "2022-06-19 14:39:13,370 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 9.8 KiB, free 412.8 MiB)\n",
      "2022-06-19 14:39:13,372 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 412.8 MiB)\n",
      "2022-06-19 14:39:13,373 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 5.4 KiB, free: 413.7 MiB)\n",
      "2022-06-19 14:39:13,373 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-19 14:39:13,373 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[36] at toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:51) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-19 14:39:13,373 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "2022-06-19 14:39:13,374 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (10.201.77.168, executor 1, partition 0, PROCESS_LOCAL, 4869 bytes) taskResourceAssignments Map()\n",
      "2022-06-19 14:39:13,387 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.201.77.168:36811 (size: 5.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:13,423 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.201.77.168:36811 (size: 44.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:13,486 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 112 ms on 10.201.77.168 (executor 1) (1/1)\n",
      "2022-06-19 14:39:13,486 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "2022-06-19 14:39:13,487 INFO scheduler.DAGScheduler: ResultStage 7 (toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:51) finished in 0.119 s\n",
      "2022-06-19 14:39:13,487 INFO scheduler.DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-19 14:39:13,487 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "2022-06-19 14:39:13,487 INFO scheduler.DAGScheduler: Job 5 finished: toPandas at /conda/lib/python3.7/site-packages/mlrun/data_types/spark.py:51, took 0.126845 s\n",
      "> 2022-06-19 14:39:13,501 [info] writing to target parquet, spark options {'path': 'v3io://projects/stocks-avia/FeatureStore/stocks/parquet/sets/stocks/1655649553501_286/', 'format': 'parquet'}\n",
      "2022-06-19 14:39:13,549 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-19 14:39:13,549 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-19 14:39:13,549 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Datetime: string, Open: double, High: double, Low: double, Close: double ... 6 more fields>\n",
      "2022-06-19 14:39:13,752 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2022-06-19 14:39:13,774 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2022-06-19 14:39:13,774 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2022-06-19 14:39:13,775 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2022-06-19 14:39:13,775 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2022-06-19 14:39:13,775 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2022-06-19 14:39:13,776 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2022-06-19 14:39:13,859 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 250.2 KiB, free 412.5 MiB)\n",
      "2022-06-19 14:39:13,874 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 412.5 MiB)\n",
      "2022-06-19 14:39:13,874 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.7 MiB)\n",
      "2022-06-19 14:39:13,875 INFO spark.SparkContext: Created broadcast 15 from save at NativeMethodAccessorImpl.java:0\n",
      "2022-06-19 14:39:13,876 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-19 14:39:13,910 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "2022-06-19 14:39:13,911 INFO scheduler.DAGScheduler: Got job 6 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-06-19 14:39:13,911 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (save at NativeMethodAccessorImpl.java:0)\n",
      "2022-06-19 14:39:13,911 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-06-19 14:39:13,911 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-06-19 14:39:13,912 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[38] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-06-19 14:39:13,940 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 257.8 KiB, free 412.2 MiB)\n",
      "2022-06-19 14:39:13,962 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 95.5 KiB, free 412.1 MiB)\n",
      "2022-06-19 14:39:13,963 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 95.5 KiB, free: 413.6 MiB)\n",
      "2022-06-19 14:39:13,963 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-19 14:39:13,963 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 5.4 KiB, free: 413.6 MiB)\n",
      "2022-06-19 14:39:13,964 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[38] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-19 14:39:13,964 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "2022-06-19 14:39:13,965 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (10.201.77.168, executor 1, partition 0, PROCESS_LOCAL, 4869 bytes) taskResourceAssignments Map()\n",
      "2022-06-19 14:39:13,966 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.201.77.168:36811 in memory (size: 5.4 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:13,976 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.201.77.168:36811 (size: 95.5 KiB, free: 413.7 MiB)\n",
      "2022-06-19 14:39:14,483 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.201.77.168:36811 (size: 44.4 KiB, free: 413.7 MiB)\n",
      "2022-06-19 14:39:15,982 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 2018 ms on 10.201.77.168 (executor 1) (1/1)\n",
      "2022-06-19 14:39:15,983 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "2022-06-19 14:39:15,983 INFO scheduler.DAGScheduler: ResultStage 8 (save at NativeMethodAccessorImpl.java:0) finished in 2.071 s\n",
      "2022-06-19 14:39:15,983 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-19 14:39:15,983 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "2022-06-19 14:39:15,984 INFO scheduler.DAGScheduler: Job 6 finished: save at NativeMethodAccessorImpl.java:0, took 2.073982 s\n",
      "2022-06-19 14:39:16,618 INFO datasources.FileFormatWriter: Write Job 8d5fbcca-093a-4c5b-9d6e-9200ccb4deaa committed.\n",
      "2022-06-19 14:39:16,623 INFO datasources.FileFormatWriter: Finished processing stats for write job 8d5fbcca-093a-4c5b-9d6e-9200ccb4deaa.\n",
      "> 2022-06-19 14:39:16,628 [info] writing to target nosql, spark options {'path': 'v3io://projects/stocks-avia/FeatureStore/stocks/nosql/sets/stocks/1655649553501_286/', 'format': 'io.iguaz.v3io.spark.sql.kv', 'key': 'ticker'}\n",
      "2022-06-19 14:39:17,332 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-06-19 14:39:17,332 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-06-19 14:39:17,332 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Datetime: string, Open: double, High: double, Low: double, Close: double ... 6 more fields>\n",
      "2022-06-19 14:39:17,340 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 250.2 KiB, free 411.9 MiB)\n",
      "2022-06-19 14:39:17,353 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 411.9 MiB)\n",
      "2022-06-19 14:39:17,354 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 44.4 KiB, free: 413.6 MiB)\n",
      "2022-06-19 14:39:17,354 INFO spark.SparkContext: Created broadcast 17 from foreachPartition at KVRelation.scala:125\n",
      "2022-06-19 14:39:17,355 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-06-19 14:39:17,408 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 44.4 KiB, free: 413.6 MiB)\n",
      "2022-06-19 14:39:17,408 INFO spark.SparkContext: Starting job: foreachPartition at KVRelation.scala:125\n",
      "2022-06-19 14:39:17,409 INFO scheduler.DAGScheduler: Got job 7 (foreachPartition at KVRelation.scala:125) with 1 output partitions\n",
      "2022-06-19 14:39:17,409 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (foreachPartition at KVRelation.scala:125)\n",
      "2022-06-19 14:39:17,410 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-06-19 14:39:17,410 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-06-19 14:39:17,410 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.201.77.168:36811 in memory (size: 44.4 KiB, free: 413.7 MiB)\n",
      "2022-06-19 14:39:17,410 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[45] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2022-06-19 14:39:17,431 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 in memory (size: 95.5 KiB, free: 413.7 MiB)\n",
      "2022-06-19 14:39:17,432 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.201.77.168:36811 in memory (size: 95.5 KiB, free: 413.8 MiB)\n",
      "2022-06-19 14:39:17,434 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 111.0 KiB, free 412.4 MiB)\n",
      "2022-06-19 14:39:17,436 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 63.1 KiB, free 412.3 MiB)\n",
      "2022-06-19 14:39:17,436 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:7079 (size: 63.1 KiB, free: 413.6 MiB)\n",
      "2022-06-19 14:39:17,436 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1388\n",
      "2022-06-19 14:39:17,437 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[45] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0))\n",
      "2022-06-19 14:39:17,437 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "2022-06-19 14:39:17,438 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (10.201.77.168, executor 1, partition 0, PROCESS_LOCAL, 4869 bytes) taskResourceAssignments Map()\n",
      "2022-06-19 14:39:17,448 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.201.77.168:36811 (size: 63.1 KiB, free: 413.7 MiB)\n",
      "2022-06-19 14:39:17,823 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.201.77.168:36811 (size: 44.4 KiB, free: 413.7 MiB)\n",
      "2022-06-19 14:39:18,668 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 1230 ms on 10.201.77.168 (executor 1) (1/1)\n",
      "2022-06-19 14:39:18,669 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "2022-06-19 14:39:18,669 INFO scheduler.DAGScheduler: ResultStage 9 (foreachPartition at KVRelation.scala:125) finished in 1.258 s\n",
      "2022-06-19 14:39:18,669 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-06-19 14:39:18,670 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "2022-06-19 14:39:18,670 INFO scheduler.DAGScheduler: Job 7 finished: foreachPartition at KVRelation.scala:125, took 1.261713 s\n",
      "> 2022-06-19 14:39:18,796 [info] ingestion task completed, targets:\n",
      "> 2022-06-19 14:39:18,796 [info] [{'name': 'parquet', 'kind': 'parquet', 'path': 'v3io:///projects/stocks-avia/FeatureStore/stocks/parquet/sets/stocks/{run_id}/', 'status': 'ready', 'updated': '2022-06-19T14:39:16.628740+00:00', 'run_id': '1655649553501_286'}, {'name': 'nosql', 'kind': 'nosql', 'path': 'v3io:///projects/stocks-avia/FeatureStore/stocks/nosql/sets/stocks/{run_id}/', 'status': 'ready', 'updated': '2022-06-19T14:39:18.730100+00:00', 'run_id': '1655649553501_286'}]\n",
      "2022-06-19 14:39:18,806 INFO server.AbstractConnector: Stopped Spark@76e2ad66{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2022-06-19 14:39:18,807 INFO ui.SparkUI: Stopped Spark web UI at http://stocks-ingest-56d51693-5ede5f817c66928b-driver-svc.default-tenant.svc:4040\n",
      "2022-06-19 14:39:18,811 INFO k8s.KubernetesClusterSchedulerBackend: Shutting down all executors\n",
      "2022-06-19 14:39:18,811 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down\n",
      "2022-06-19 14:39:18,817 WARN k8s.ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n",
      "2022-06-19 14:39:18,982 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2022-06-19 14:39:18,992 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2022-06-19 14:39:18,993 INFO storage.BlockManager: BlockManager stopped\n",
      "2022-06-19 14:39:18,997 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2022-06-19 14:39:19,003 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2022-06-19 14:39:19,011 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "> 2022-06-19 14:39:19,298 [info] run executed, status=completed\n",
      "2022-06-19 14:39:19,836 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "2022-06-19 14:39:19,837 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-12770e9d-1bd5-4d0e-8c43-4c90852042eb\n",
      "2022-06-19 14:39:19,841 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-d6c2ae3e-23a5-438e-a577-f9aa7a15f85e/spark-71d4e150-5d05-4507-b2c5-6f96ee9dea4e\n",
      "2022-06-19 14:39:19,844 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-d6c2ae3e-23a5-438e-a577-f9aa7a15f85e/spark-71d4e150-5d05-4507-b2c5-6f96ee9dea4e/pyspark-d6a67069-fb17-401e-b90d-3eebe6f8ec16\n",
      "final state: completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>stocks-avia</td>\n",
       "      <td><div title=\"0ad6d9e0a5034930a12ab6b267d84db0\"><a href=\"https://dashboard.default-tenant.app.dev39.lab.iguazeng.com/mlprojects/stocks-avia/jobs/monitor/0ad6d9e0a5034930a12ab6b267d84db0/overview\" target=\"_blank\" >...67d84db0</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Jun 19 14:38:39</td>\n",
       "      <td>completed</td>\n",
       "      <td>stocks-ingest</td>\n",
       "      <td><div class=\"dictlist\">job-type=feature-ingest</div><div class=\"dictlist\">feature-set=store://feature-sets/stocks-avia/stocks</div><div class=\"dictlist\">v3io_user=avia</div><div class=\"dictlist\">kind=spark</div><div class=\"dictlist\">owner=avia</div><div class=\"dictlist\">mlrun/client_version=1.0.4</div><div class=\"dictlist\">mlrun/job=stocks-ingest-56d51693</div><div class=\"dictlist\">host=stocks-ingest-56d51693-driver</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">infer_options=63</div><div class=\"dictlist\">overwrite=None</div><div class=\"dictlist\">featureset=store://feature-sets/stocks-avia/stocks</div><div class=\"dictlist\">source={'kind': 'csv', 'name': 'mycsv', 'path': 'v3io:///projects/input.csv'}</div><div class=\"dictlist\">targets=None</div></td>\n",
       "      <td><div class=\"dictlist\">featureset=store://feature-sets/stocks-avia/stocks:latest</div></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"resulta40853b6-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"resulta40853b6-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"resulta40853b6\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"resulta40853b6-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.dev39.lab.iguazeng.com/mlprojects/stocks-avia/jobs/monitor/0ad6d9e0a5034930a12ab6b267d84db0/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-06-19 14:39:19,912 [info] run executed, status=completed\n"
     ]
    }
   ],
   "source": [
    "my_func = code_to_function(\"func\", kind=\"spark\")\n",
    "\n",
    "my_func.with_driver_requests(cpu=1, mem=\"1G\")\n",
    "my_func.with_executor_requests(cpu=1, mem=\"1G\")\n",
    "my_func.with_igz_spark()\n",
    "\n",
    "# Enables using the default image (can be replace with specifying a specific image with .spec.image)\n",
    "my_func.spec.use_default_image = True\n",
    "\n",
    "# Not a must - default: 1\n",
    "my_func.spec.replicas = 2\n",
    "\n",
    "# If needed, sparkConf can be modified like this:\n",
    "# my_func.spec.spark_conf['spark.specific.config.key'] = 'value'\n",
    "\n",
    "config = fstore.RunConfig(local=False, function=my_func, handler=\"ingest_handler\")\n",
    "quotes_df = fstore.ingest(feature_set, source, run_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9158ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"quotes.*\",\n",
    "]\n",
    "\n",
    "vector = fstore.FeatureVector(\"stocks-vec\",features=features,description=\"this is my vector\",with_indexes=True)\n",
    "resp = fstore.get_offline_features(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a30b3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = resp.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7556f0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>ticker2onehot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>2022-06-06 09:30:00</td>\n",
       "      <td>129.380005</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>129.169998</td>\n",
       "      <td>129.255005</td>\n",
       "      <td>61282</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>2022-06-06 09:35:00</td>\n",
       "      <td>129.181503</td>\n",
       "      <td>129.399994</td>\n",
       "      <td>129.039993</td>\n",
       "      <td>129.399994</td>\n",
       "      <td>17839</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>2022-06-06 09:40:00</td>\n",
       "      <td>129.429993</td>\n",
       "      <td>129.889999</td>\n",
       "      <td>129.300003</td>\n",
       "      <td>129.399994</td>\n",
       "      <td>6999</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>2022-06-06 09:45:00</td>\n",
       "      <td>129.509995</td>\n",
       "      <td>129.509995</td>\n",
       "      <td>128.880005</td>\n",
       "      <td>128.880005</td>\n",
       "      <td>20361</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>2022-06-06 09:50:00</td>\n",
       "      <td>128.794998</td>\n",
       "      <td>129.009995</td>\n",
       "      <td>128.660004</td>\n",
       "      <td>128.990005</td>\n",
       "      <td>9078</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2022-06-08 12:50:00</td>\n",
       "      <td>148.059998</td>\n",
       "      <td>148.139999</td>\n",
       "      <td>147.979996</td>\n",
       "      <td>148.039993</td>\n",
       "      <td>348492</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2022-06-08 12:55:00</td>\n",
       "      <td>148.029999</td>\n",
       "      <td>148.259995</td>\n",
       "      <td>147.971207</td>\n",
       "      <td>148.229996</td>\n",
       "      <td>296906</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2022-06-08 13:00:00</td>\n",
       "      <td>148.220001</td>\n",
       "      <td>148.240005</td>\n",
       "      <td>147.910004</td>\n",
       "      <td>148.014999</td>\n",
       "      <td>425430</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2022-06-08 13:05:00</td>\n",
       "      <td>148.020004</td>\n",
       "      <td>148.190002</td>\n",
       "      <td>148.005005</td>\n",
       "      <td>148.149994</td>\n",
       "      <td>259544</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2022-06-08 13:10:33</td>\n",
       "      <td>148.119904</td>\n",
       "      <td>148.119904</td>\n",
       "      <td>148.119904</td>\n",
       "      <td>148.119904</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>804 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime        Open        High         Low       Close  \\\n",
       "ticker                                                                        \n",
       "A       2022-06-06 09:30:00  129.380005  130.000000  129.169998  129.255005   \n",
       "A       2022-06-06 09:35:00  129.181503  129.399994  129.039993  129.399994   \n",
       "A       2022-06-06 09:40:00  129.429993  129.889999  129.300003  129.399994   \n",
       "A       2022-06-06 09:45:00  129.509995  129.509995  128.880005  128.880005   \n",
       "A       2022-06-06 09:50:00  128.794998  129.009995  128.660004  128.990005   \n",
       "...                     ...         ...         ...         ...         ...   \n",
       "AAPL    2022-06-08 12:50:00  148.059998  148.139999  147.979996  148.039993   \n",
       "AAPL    2022-06-08 12:55:00  148.029999  148.259995  147.971207  148.229996   \n",
       "AAPL    2022-06-08 13:00:00  148.220001  148.240005  147.910004  148.014999   \n",
       "AAPL    2022-06-08 13:05:00  148.020004  148.190002  148.005005  148.149994   \n",
       "AAPL    2022-06-08 13:10:33  148.119904  148.119904  148.119904  148.119904   \n",
       "\n",
       "        Volume ticker2onehot  \n",
       "ticker                        \n",
       "A        61282             A  \n",
       "A        17839             A  \n",
       "A         6999             A  \n",
       "A        20361             A  \n",
       "A         9078             A  \n",
       "...        ...           ...  \n",
       "AAPL    348492          AAPL  \n",
       "AAPL    296906          AAPL  \n",
       "AAPL    425430          AAPL  \n",
       "AAPL    259544          AAPL  \n",
       "AAPL         0          AAPL  \n",
       "\n",
       "[804 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276e3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
