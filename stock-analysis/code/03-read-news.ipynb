{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape news and Analyse sentiments\n",
    "This notebook shows an example of scraping news articles linked to specific traded companies and utilizing our predeployed sentiment analysis model server to predict the sentiment of the author towards said companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the nuclio-jupyter package is not installed run !pip install nuclio-jupyter\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio env -c V3IO_ACCESS_KEY=${V3IO_ACCESS_KEY}\n",
    "%nuclio env -c V3IO_USERNAME=${V3IO_USERNAME}\n",
    "%nuclio env -c V3IO_API=${V3IO_API}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "pip install beautifulsoup4\n",
    "pip install pandas\n",
    "pip install v3io_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio config \n",
    "kind = \"nuclio\"\n",
    "spec.build.baseImage = \"mlrun/mlrun:0.6.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "import pandas as pd\n",
    "import v3io_frames as v3f\n",
    "from unicodedata import normalize\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import mlrun.feature_store as fs\n",
    "import mlrun\n",
    "from mlrun.datastore.targets import ParquetTarget\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_news_page(stock_string):\n",
    "    request = Request('https://www.investing.com/equities/' + stock_string + '-news', headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    content = urlopen(request).read()\n",
    "    return bs(content, 'html.parser')\n",
    "\n",
    "def get_internal_article_links(page,sym):\n",
    "    if(sym == \"INTC\"): # in webpage of INTEL, the html tags are arranged differently\n",
    "        news = page.find_all('div', attrs={'class': 'js-search-ga-items articles mediumTitle1'})[0]\n",
    "    else:\n",
    "        news = page.find_all('div', attrs={'class': 'mediumTitle1'})[1]\n",
    "    articles = news.find_all('article', attrs={'class': 'js-article-item articleItem'})\n",
    "    return ['https://www.investing.com' + a.find('a').attrs['href'] for a in articles]\n",
    "\n",
    "def get_article_page(article_link):\n",
    "    request = Request(article_link, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    content = urlopen(request).read()\n",
    "    return bs(content, 'html.parser')\n",
    "\n",
    "def clean_paragraph(paragraph):\n",
    "    paragraph = re.sub(r'\\(http\\S+', '', paragraph)\n",
    "    paragraph = re.sub(r'\\([A-Z]+:[A-Z]+\\)', '', paragraph)\n",
    "    paragraph = re.sub(r'[\\n\\t\\s\\']', ' ', paragraph)\n",
    "    return normalize('NFKD', paragraph)    \n",
    "\n",
    "def extract_text(article_page):\n",
    "    text_tag = article_page.find('div', attrs={'class': 'WYSIWYG articlePage'})\n",
    "    paragraphs = text_tag.find_all('p')\n",
    "    text = '\\n'.join([clean_paragraph(p.get_text()) for p in paragraphs[:-1]])\n",
    "    return text\n",
    "\n",
    "def get_publish_time(article):\n",
    "    tag = article.find('script',{\"type\" : \"application/ld+json\"}).contents[0]\n",
    "    tag_dict = json.loads(str(tag))\n",
    "    dateModified = tag_dict[\"dateModified\"]\n",
    "    return datetime.strftime(datetime.strptime(dateModified, '%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def get_score(paragraph_scores):\n",
    "    return sum([score - 1 for score in paragraph_scores['outputs']]) / len(paragraph_scores)\n",
    "\n",
    "def get_article_scores(context, articles, endpoint):\n",
    "    scores = [] \n",
    "    endpoint = endpoint + \"/v2/models/model1/predict\"\n",
    "    for i, article in enumerate(articles):\n",
    "        context.logger.info(f'getting score for article {i + 1}\\\\{len(articles)}')\n",
    "        event_data = {'inputs': article.split('\\n')}\n",
    "        resp = requests.put(endpoint, json=json.dumps(event_data))\n",
    "        scores.append(get_score(json.loads(resp.text)))\n",
    "    return scores\n",
    "\n",
    "def construct_dataframe(sentiments, items,times):\n",
    "    tickers = [x[0] for x in items]\n",
    "    stock_sent = pd.DataFrame({\"symbol\": tickers, \"sentiment\": sentiments, \"last_reaction\": times})\n",
    "    return stock_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    context.logger.info(\"init news reader context\")\n",
    "    setattr(context, 'PROJECT_NAME', os.getenv('PROJECT_NAME', 'stocks-' + os.getenv('V3IO_USERNAME')))\n",
    "    mlrun.set_environment(project = context.PROJECT_NAME)\n",
    "    \n",
    "    # Declaring feature set\n",
    "    stocks_sent_set = fs.FeatureSet(\"news\", entities=[fs.Entity(\"symbol\")],timestamp_key=\"Datetime\")\n",
    "    stocks_sent_set.set_targets(targets=[ParquetTarget(name=\"news\",partitioned=True,time_partitioning_granularity=\"day\")]\n",
    "                                ,with_defaults=False)\n",
    "    setattr(context, 'stock_feature_set', stocks_sent_set)\n",
    "    \n",
    "    # saving timestamps to know what to ingest\n",
    "    last_trade_times = {}\n",
    "    setattr(context, 'last_trade_times', last_trade_times)\n",
    "    \n",
    "    v3io_framesd = os.getenv('V3IO_FRAMESD', 'framesd:8081')\n",
    "    token = os.getenv('TOKEN', '')\n",
    "    client = v3f.Client(v3io_framesd, container=os.getenv('V3IO_CONTAINER', 'users'), token=token)\n",
    "    setattr(context, 'v3c', client)\n",
    "    # Create V3IO Tables and add reference to context\n",
    "    setattr(context, 'stocks_stream', os.getenv('STOCKS_STREAM', os.getenv('V3IO_USERNAME') + '/stocks/stocks_stream'))\n",
    "    context.v3c.create(backend='stream', table=context.stocks_stream, if_exists=1)\n",
    "\n",
    "    setattr(context, 'stocks_tsdb', os.getenv('STOCKS_TSDB_TABLE', os.getenv('V3IO_USERNAME') + '/stocks/stocks_tsdb'))\n",
    "    context.v3c.create(backend='tsdb', table=context.stocks_tsdb, rate='1/s', if_exists=1)\n",
    "\n",
    "    setattr(context, 'sentiment_model_endpoint',\n",
    "            os.getenv('SENTIMENT_MODEL_ENDPOINT', '')) # in the '' should be the sentiment-analysis model endpoint\n",
    "    context.logger.info(f\"set sentiment_model_endpoint {context.sentiment_model_endpoint}\")\n",
    "    sym_to_url = {'GOOGL': 'google-inc', 'MSFT': 'microsoft-corp', 'AMZN': 'amazon-com-inc',\n",
    "                  'AAPL': 'apple-computer-inc', 'INTC' : 'intel-corp'}\n",
    "    setattr(context, 'sym_to_url', sym_to_url)\n",
    "    setattr(context, 'stocks_kv', os.getenv('STOCKS_KV', os.getenv('V3IO_USERNAME') + '/stocks/stocks_kv'))\n",
    "    context.logger.info('end init context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "    '''\n",
    "    Extracting news from investing.com using beautifulsoup, \n",
    "    '''\n",
    "    context.logger.info(f'Getting news about {context.sym_to_url}')\n",
    "    syms = []\n",
    "    contents = []\n",
    "    links = []\n",
    "    times = []\n",
    "    sentiments = []\n",
    "    all_records = []\n",
    "    for sym, url_string in list(context.sym_to_url.items()):\n",
    "        context.logger.info(f'Getting news about {sym}')\n",
    "        news_page = get_stock_news_page(url_string)\n",
    "        article_links = get_internal_article_links(news_page,sym)\n",
    "        article_pages = [get_article_page(link) for link in article_links]\n",
    "        articles = [extract_text(article_page) for article_page in article_pages]\n",
    "        curr_sentiments = get_article_scores(context, articles, context.sentiment_model_endpoint)\n",
    "        curr_times = [get_publish_time(article_page) for article_page in article_pages]\n",
    "        sentiments += curr_sentiments\n",
    "        times += curr_times\n",
    "        time = datetime.strptime(curr_times[0],\"%Y-%m-%d %H:%M:%S\")\n",
    "        last = context.last_trade_times.get(sym)\n",
    "        if not last:\n",
    "            last = datetime(1990,1,1)\n",
    "        for article, link, sentiment, time in zip(articles, article_links, curr_sentiments, curr_times):\n",
    "            record = {\n",
    "                'content': article,\n",
    "                'time': time,\n",
    "                'symbol': sym,\n",
    "                'link': link,\n",
    "                'sentiment': sentiment\n",
    "            }\n",
    "            context.v3c.execute('stream', context.stocks_stream, 'put', args={'data': json.dumps(record)})\n",
    "            timestamped_record = record.copy()\n",
    "            timestamped_record[\"time\"] = datetime.strptime(record[\"time\"],\"%Y-%m-%d %H:%M:%S\")\n",
    "            if(last):\n",
    "                if(timestamped_record.get(\"time\")>last):\n",
    "                    all_records.append(timestamped_record)\n",
    "                    context.last_trade_times[sym] = timestamped_record.get(\"time\")\n",
    "            else:\n",
    "                all_records.append(timestamped_record)\n",
    "                context.last_trade_times[sym] = timestamped_record.get(\"time\")\n",
    "            syms.append(sym)\n",
    "            contents.append(article)\n",
    "            links.append(link)\n",
    "            \n",
    "        context.v3c.execute('kv', context.stocks_kv, command='update', args={'key': sym,\n",
    "                                                                         'expression': f\"SET sentiment='{sentiments[-1]}';last_reaction='{times[-1]}'\"})\n",
    "    all_records = pd.DataFrame(all_records)\n",
    "    if(all_records.shape[0]>0):\n",
    "        all_records.columns = [\"Content\",\"Datetime\",\"symbol\",\"Link\",\"Sentiment\"]\n",
    "        # Localizing the datetime\n",
    "        all_records[\"Datetime\"] = all_records[\"Datetime\"].dt.tz_localize('UTC')\n",
    "        # writing to featureset\n",
    "        context.logger.info(f\"Writing new dataframe with shape {all_records.shape} to feature store\")\n",
    "        fs.ingest(context.stock_feature_set, all_records, infer_options=fs.InferOptions.default(),\n",
    "                 overwrite=False)\n",
    "    else:\n",
    "        context.logger.info(\"No new data to ingest\")\n",
    "\n",
    "    if len(sentiments) > 0:\n",
    "        df = pd.DataFrame.from_dict({'sentiment': sentiments,\n",
    "                                     'time': times,\n",
    "                                     'symbol': syms})\n",
    "        df = df.set_index(['time', 'symbol'])\n",
    "        df.index = df.index.set_levels([pd.to_datetime(df.index.levels[0]), df.index.levels[1]])\n",
    "        df = df.sort_index(level=0, axis=0)\n",
    "        context.v3c.write(backend='tsdb', table=context.stocks_tsdb, dfs=df)\n",
    "    return \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuclio import Event\n",
    "event = Event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = handler(context, event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please run the cells below in order to get the SENTIMENT_MODEL_ENDPOINT endopoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function,auto_mount\n",
    "import os\n",
    "import nuclio\n",
    "# Export bare function\n",
    "fn = code_to_function('news-reader',\n",
    "                      handler='handler')\n",
    "fn.export('02-read-news.yaml')\n",
    "\n",
    "# Set parameters for current deployment\n",
    "fn.add_trigger('cron', nuclio.triggers.CronTrigger('10s'))\n",
    "fn.set_envs({'V3IO_CONTAINER': 'users',\n",
    "             'STOCKS_STREAM': os.getenv('V3IO_USERNAME') + '/stocks/stocks_stream',\n",
    "             'STOCKS_TSDB_TABLE': os.getenv('V3IO_USERNAME') + '/stocks/stocks_tsdb',\n",
    "             'SENTIMENT_MODEL_ENDPOINT': '', # make sure you insert the right endpoint when running test\n",
    "             'PROJECT_NAME' :\"stocks-\" + os.getenv('V3IO_USERNAME')})\n",
    "fn.spec.max_replicas = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr = fn.deploy(project= \"stocks-\" + os.getenv('V3IO_USERNAME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl {addr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in case running this notebook without running the entire project (means sentiment analysis serving function isn't depolyed) run the cells below to deploy the sentiment analysis serving function, then copy the given url after depolyment to the sentiment analysis endpoint in the set_enviornemt section above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying sentiment analysis serving function\n",
    "import mlrun\n",
    "fn = mlrun.import_function(url = \"hub://sentiment_analysis_serving\")\n",
    "fn.apply(mlrun.auto_mount())\n",
    "# make sure you have the model - if not - download it from the project notebook\n",
    "fn.add_model(model_path=\"/User/test/demos/stock-analysis/models/model.pt\",key=\"model1\",class_name=\"SentimentClassifierServing\")\n",
    "addr = fn.deploy(project=\"stocks-\" + os.getenv('V3IO_USERNAME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl {addr}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
